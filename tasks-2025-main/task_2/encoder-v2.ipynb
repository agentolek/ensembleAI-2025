{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-15T23:57:21.633916Z",
     "start_time": "2025-03-15T23:57:20.080035Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchvision"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T23:57:28.834685Z",
     "start_time": "2025-03-15T23:57:28.831790Z"
    }
   },
   "cell_type": "code",
   "source": "output_dim = 1024",
   "id": "895d4bc429d3971",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T23:57:41.603765Z",
     "start_time": "2025-03-15T23:57:41.252035Z"
    }
   },
   "cell_type": "code",
   "source": "model = torchvision.models.resnet50(pretrained=True)",
   "id": "e26044deae622d22",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/werkaj/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/werkaj/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T23:59:24.131449Z",
     "start_time": "2025-03-15T23:59:24.110431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.conv1 = torch.nn.Conv2d(3, 64, kernel_size = 3, stride=1, padding=1, bias=False)\n",
    "model.maxpool = torch.nn.Identity()\n",
    "model.fc = torch.nn.Linear(2048, output_dim)\n",
    "print(model)"
   ],
   "id": "d5bbc6ee33b0fad2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): Identity()\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1024, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:03:09.317374Z",
     "start_time": "2025-03-16T00:03:09.235559Z"
    }
   },
   "cell_type": "code",
   "source": "from example_submission import TaskDataset",
   "id": "e960ef53ca86ebe4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:24:47.812921Z",
     "start_time": "2025-03-16T00:24:47.806764Z"
    }
   },
   "cell_type": "code",
   "source": "torch.serialization.safe_globals([TaskDataset])",
   "id": "44228019eb3bede",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.serialization.safe_globals at 0x71f8f4412b60>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:27:03.184479Z",
     "start_time": "2025-03-16T00:27:03.003853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.load(\"ModelStealingPub.pt\", weights_only=False)\n",
    "print(data)"
   ],
   "id": "49a4bddbf7a39bbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<example_submission.TaskDataset object at 0x71f8f44804f0>\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:28:12.245694Z",
     "start_time": "2025-03-16T00:28:12.241140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.2980, 0.2962, 0.2987], [0.2886, 0.2875, 0.2889])\n",
    "])"
   ],
   "id": "8acee9d055d3fc89",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:05:09.862854Z",
     "start_time": "2025-03-16T00:05:09.859696Z"
    }
   },
   "cell_type": "code",
   "source": "data.transform = data_transforms",
   "id": "2e1046826f40ac27",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:05:12.776063Z",
     "start_time": "2025-03-16T00:05:12.770907Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "6a975fc60ca17450",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<example_submission.TaskDataset at 0x71f904733670>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:05:19.794420Z",
     "start_time": "2025-03-16T00:05:19.788151Z"
    }
   },
   "cell_type": "code",
   "source": "data.transform",
   "id": "4dd958d609237dfc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.298, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889])\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:05:53.808227Z",
     "start_time": "2025-03-16T00:05:53.803364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 32\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "id": "436524f9a0c1a6e1",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:06:18.675877Z",
     "start_time": "2025-03-16T00:06:18.116315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch in data_loader:\n",
    "    print(batch)"
   ],
   "id": "70c4fcc79cf8f502",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 32, 32] doesn't match the broadcast shape [3, 32, 32]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m data_loader:\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(batch)\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 708\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    709\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    710\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    712\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    713\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    714\u001B[0m ):\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    762\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    763\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 764\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    766\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/tasks-2025-main/task_2/example_submission.py:36\u001B[0m, in \u001B[0;36mTaskDataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     34\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimgs[index]\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 36\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[index]\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m id_, img, label\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:277\u001B[0m, in \u001B[0;36mNormalize.forward\u001B[0;34m(self, tensor)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, tensor: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    270\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;124;03m        Tensor: Normalized Tensor image.\u001B[39;00m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:350\u001B[0m, in \u001B[0;36mnormalize\u001B[0;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensor, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg should be Tensor Image. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensor)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 350\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmean\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ensembleAI-2025/.venv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:928\u001B[0m, in \u001B[0;36mnormalize\u001B[0;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[1;32m    926\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m std\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    927\u001B[0m     std \u001B[38;5;241m=\u001B[39m std\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 928\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdiv_(std)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: output with shape [1, 32, 32] doesn't match the broadcast shape [3, 32, 32]"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:08:59.533361Z",
     "start_time": "2025-03-16T00:08:59.511459Z"
    }
   },
   "cell_type": "code",
   "source": "print(data.imgs[0].shape) ",
   "id": "6de87e2bbe437144",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimgs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m) \n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Image' object has no attribute 'shape'"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:09:53.422643Z",
     "start_time": "2025-03-16T00:09:53.418455Z"
    }
   },
   "cell_type": "code",
   "source": "pic = data.imgs[0]",
   "id": "7f73cdac4db985d0",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:09:55.473016Z",
     "start_time": "2025-03-16T00:09:55.457580Z"
    }
   },
   "cell_type": "code",
   "source": "pic",
   "id": "6e8eaa6fd05c30a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ],
      "image/png": "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",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwC0GESQyhkclSOAVIbJPOfw/OpUl/0YRyRt5a5dkHHygDqP6+9SW9lxB5aAli5TL/cHIyfXvUcqPJa7oXADAGURZYdOO59K7D0xQoijuFeYRFGRMjJ4yTjp15+lXo5IZIZDNBuVCGAEYwMg9v8APWq0cQng3XMikS/fwAMsM84H1zTU8uORbgmRiWwowQhz0zn1HH4UMCSN2t5QPKy23CyENjB5G0+nWkHlW4M6vJh2ypi4ycg/kDx+FIrql0WKgHIK7mP3QOMZ6cdverFtbK6L5ibo4jkg8fp7+/rSAcIpRbMFUNIku1sgEHgHnnjJ60TXHk3CCcR7cbgwkIyp6kcHIGP0qoVknV3jiJjLjeN2NmTnJx2HPNJdARSjeEIhXDKzcEdxx/vUWEf/2Q=="
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:12:02.047612Z",
     "start_time": "2025-03-16T00:12:02.044305Z"
    }
   },
   "cell_type": "code",
   "source": "pic = data.imgs[1]",
   "id": "9e4038a41861d754",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:12:02.722137Z",
     "start_time": "2025-03-16T00:12:02.717117Z"
    }
   },
   "cell_type": "code",
   "source": "pic",
   "id": "1d73b2aedbec1918",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ],
      "image/png": "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",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD0wRQQREzAAY61k3pSMnaRj+Eegqw3irT0ibqpIxyOKx5r+2u5spOhwepUgfrXVFPqdsU76kb3Fu26O4JcZxjFQS2IUF7G4mt2bldvT8jUq2YkYYuIgoOcsOn0qwV+XypW3x9ii8fzqzXma2Kf2Oznm3l1I68Hir4igt41FvEDnq3WsCa1ZYzHFbI4fIb975eB9QM1Ytn+z2yQCYRBOP8AWM3H1NBTj5mu6Ff3m390TnryKC25AoGUY496htI2JDGb5T2xkVYkh8iPzQWZhwAMCkZs/9k="
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:16:16.934619Z",
     "start_time": "2025-03-16T00:16:16.930202Z"
    }
   },
   "cell_type": "code",
   "source": "pic = data.__getitem__(0)[1]",
   "id": "e93698a3600c6314",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:16:17.426890Z",
     "start_time": "2025-03-16T00:16:17.422014Z"
    }
   },
   "cell_type": "code",
   "source": "pic.shape",
   "id": "9483301ed30f7a55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T00:16:22.174007Z",
     "start_time": "2025-03-16T00:16:22.168271Z"
    }
   },
   "cell_type": "code",
   "source": "pic",
   "id": "78ab6430bbdd8654",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.3046, 1.0328, 1.2503,  ..., 1.1551, 0.9649, 1.0057],\n",
       "         [1.1280, 1.1415, 1.6171,  ..., 1.3454, 1.4813, 1.8210],\n",
       "         [1.2095, 1.1959, 1.4405,  ..., 1.7123, 1.8617, 1.6715],\n",
       "         ...,\n",
       "         [1.4948, 1.4813, 1.2638,  ..., 1.6987, 1.5220, 1.1687],\n",
       "         [1.2638, 1.4948, 1.3182,  ..., 1.6987, 1.2095, 1.1008],\n",
       "         [1.3725, 1.7394, 1.4133,  ..., 1.3997, 1.2231, 1.0600]],\n",
       "\n",
       "        [[0.6338, 0.3747, 0.6611,  ..., 0.4156, 0.2792, 0.3610],\n",
       "         [0.4292, 0.4838, 1.0840,  ..., 0.6066, 0.8657, 1.3431],\n",
       "         [0.4974, 0.4974, 0.8248,  ..., 1.0158, 1.2477, 1.0567],\n",
       "         ...,\n",
       "         [0.9203, 0.8248, 0.5520,  ..., 1.0567, 0.9203, 0.4974],\n",
       "         [0.5384, 0.7839, 0.5656,  ..., 1.0567, 0.4974, 0.3474],\n",
       "         [0.7430, 1.1249, 0.6748,  ..., 0.6884, 0.4838, 0.1974]],\n",
       "\n",
       "        [[1.2465, 1.0565, 1.2737,  ..., 1.0972, 0.9479, 1.0293],\n",
       "         [1.0972, 1.1515, 1.5587,  ..., 1.2465, 1.4094, 1.7352],\n",
       "         [1.1244, 1.1379, 1.3823,  ..., 1.5044, 1.6673, 1.5180],\n",
       "         ...,\n",
       "         [1.4366, 1.3823, 1.1787,  ..., 1.5316, 1.4501, 1.1379],\n",
       "         [1.1787, 1.3551, 1.1922,  ..., 1.5316, 1.1244, 1.0158],\n",
       "         [1.3008, 1.5859, 1.2737,  ..., 1.2737, 1.1379, 0.9208]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "767c33a3bc243ce5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
